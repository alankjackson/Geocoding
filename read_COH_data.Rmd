---
title: "Read COH data"
author: "Alan Jackson"
date: "January 5, 2020"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(leaflet)

path <- "/home/ajackson/Rprojects/Geocoding/"

deg_to_ft <-  363633.62 # feet per degree at 29 degrees

knitr::opts_chunk$set(echo = TRUE)
```

## Read in csv file

```{r read in file}

DF <- read_csv(paste0(path, "COH_ADDRESS_POINTS__PDD.csv"),
               col_types = "dddddcccccccccccddccccccccccccccc")

```

##  Do some cleanup and recast some variables

a handful of zipcodes end in *
several addresses contain a letter at the end like "B"
etc

```{r look at data}

check_num <- function(name_a, a){paste("Range", name_a, toString(range(a, na.rm=TRUE), sep=", "), "number of NA's", sum(is.na(a)))}

check_char <- function(name_a, a){print(paste("Variable:", name_a))
  print(head(unique(a), 25))
  print(paste("Number of NA's:", sum(is.na(a))))
  print(paste("Number Unique:", length(unique(a))))}

check_notnum <- function(name_a, a){print(paste("Variable:", name_a, "non-numerics =", length(a[grepl("\\D",a)])))
  head(a[grepl("\\D",a)])
  }

check_num("X", DF$X)
print("-----------------------")
check_num("Y", DF$Y)
print("-----------------------")
check_num("OBJECTID", DF$OBJECTID)
print("-----------------------")
check_num("ID", DF$ID)
print("-----------------------")
check_num("ADDR_ID", DF$ADDR_ID)
print("-----------------------")

check_char("TYPES", DF$TYPES)
print("-----------------------")
check_char("STATUS", DF$STATUS)
print("-----------------------")
check_char("STREET_NUM", DF$STREET_NUM)
check_notnum("STREET_NUM", DF$STREET_NUM)
print("-----------------------")

check_char("FRACTION", DF$FRACTION)
print("-----------------------")
check_char("PREFIX", DF$PREFIX)
print("-----------------------")
check_char("STREET_NAME", DF$STREET_NAME)
print("-----------------------")
check_char("SUFFIX", DF$SUFFIX)
print("-----------------------")
check_char("STREET_TYPE", DF$STREET_TYPE)
print("-----------------------")
check_char("STATE", DF$STATE)
print("-----------------------")
check_char("CITY", DF$CITY)
print("-----------------------")
check_char("ZIPCODE", DF$ZIPCODE)
check_notnum("ZIPCODE", DF$ZIPCODE)
print("-----------------------")
check_num("X_COORD", DF$X_COORD)
print("-----------------------")
check_num("Y_COORD", DF$Y_COORD)
print("-----------------------")
check_char("LOT", DF$LOT)
print("-----------------------")
check_char("BLOCK", DF$BLOCK)
print("-----------------------")
check_char("SOURCES", DF$SOURCES)
print("-----------------------")

```

## Data Observations

X and Y seem more reliable than X_COORD and Y_COORD. The range is correct, and there are no NA's. But do they differ?

OBJECTID looks good - the range more than covers the number of records, and no NA's. Are they unique?

ID has lots of NA's. Not sure what it is for.

Not sure what ADDR_ID is, but no NA's.

TYPES possibly useful, but largely NA dominated. 90% NA.

STATUS seems important, no NA's. Need to figure out what it means.

STREET_NUM has only 1 NA (good!), but almost 10,000 entries with an appended character string like "A" or "#1".

FRACTION is all over the map. Probably not useful.

PREFIX has issues. Case problems, and what is "C"?

STREET_NAME is good - no NA's.

SUFFIX is okay except for "Y" and "w".

STREET_TYPE is a problem. Too many. 109 street types? Really?

STATE has one NA. Seriously?

CITY has a lot of entries - 46! But only 2 NA's.

ZIPCODE: no NA's but 14 "7737*".

X_COORD, Y_COORD range wrong, needs further checking.

LOT don't know what it is, but doesn't look useful, to me.

BLOCK is NA about half the time, but appears to designate blocks when present.

SOURCES nearly always present, but not sure how I would use it. It may tell which duplicate address to use.

Rest of the fields are internal things like date updated, who did the update, etc.

##  Data cleanup

Let's now go through and clean up the data. I'll just delete records which are unrecoverable (bad zipcode, street number NA, street number not a number? )

Also look at other issues that the summaries uncovered.

```{r cleaning one}

# drop fields I don't care about

DF_work <- DF %>% select(X, Y, OBJECTID, TYPES, STATUS,
                         STREET_NUM, PREFIX, STREET_NAME,
                         SUFFIX, STREET_TYPE, CITY,
                         ZIPCODE, X_COORD, Y_COORD, BLOCK, SOURCES)

# eliminate City, Street_number with NA

DF_work <- DF_work %>% filter(CITY!="NA", STREET_NUM!="NA")

# eliminate non-numeric zips

DF_work <- DF_work %>% filter(!grepl("\\D", ZIPCODE))


```

##  Look at coordinates

Let's look at X,Y vs X_COORD and Y_COORD. How different are they?

```{r Coordinatefest}

DF_work <- 
  DF_work %>% 
  filter(!is.na(X_COORD), !is.na(Y_COORD)) 
  
  DF_work %>% 
  mutate(xdiff=X-X_COORD, ydiff=Y-Y_COORD) %>% 
  select(X, Y, xdiff, ydiff) %>% 
  ggplot() +
    geom_histogram(aes(x = xdiff), fill = "red", alpha = 0.2) + 
    geom_histogram(aes(x = ydiff), fill = "blue", alpha = 0.2) 

  DF_work %>% 
  ggplot() +
    geom_histogram(aes(x = X_COORD), fill = "red", alpha = 0.2) + 
    geom_histogram(aes(x = Y_COORD), fill = "blue", alpha = 0.2) 
```

I see. X_COORD and Y_COORD are actual projected X,Y values, probably Lambert, while X,Y are actually lat, long values. I'll keep the projected coordinates for now. But delete the NA's.

## Objectid - unique?

```{r objectid}

n_distinct(DF_work$OBJECTID)
nrow(DF_work)

```

Yep. Unique. A very useful field.

## Status

Let's count up each status

```{r status counts}

DF_work %>% group_by(STATUS) %>% tally(sort=TRUE)

```

What do they mean?

COH Permanent, Center Point, and County are all good addresses. PRE means preliminary - probably will be made one of the first three, later. Abandon means used to be a valid address, but no longer is. For example, 2050 Main might have the building razed and replaced with two now at 2050A and 2050B. 2050 gets abandoned.

UTA - no clue. Seems to be valid, but not tied to a particular structure.

Reverse Geocode - Not sure, suspect it means "interpolated address".

Out of Range - really means out of sequence, e.g., 5,7,4,9 4 would be out of range. Drop these if doing any interpolation!

## STREET_NUM

What to do about those annoying extras added to the street numbers, like "A" or "#5"? Since my ultimate goal is to geocode by block range,
all I need to calculate is the block range, and something approximating
the center of the block. Should I use the average location, the median, or perhaps the midpoint between the min and max? I think initially I might calculate all three and compare, just to see how they each work. For all of these, I can drop the non-numeric and it will affect nothing. So that seems to be the answer.

```{r cleanup street number}

# remove anything following numerics that start with a non-numeric

DF_work %>% mutate(STREET_NUMa=str_extract(STREET_NUM, "^\\d+")) -> 
  DF_work

check_char("STREET_NUMa", DF_work$STREET_NUMa)
check_notnum("STREET_NUMa", DF_work$STREET_NUMa)

# Make STREET_NUMa numeric to ease later calculations

DF_work$STREET_NUMa <- as.numeric(DF_work$STREET_NUMa)

```

## PREFIX

This should be N, S, E, W, but also has w, e, and C as values. Clearly I just need to uppercase w and e, but what is "C"? C should be NA. I looked at the map for that street.

```{r prefix}

DF_work %>% group_by(PREFIX) %>% tally(sort=TRUE)

DF_work[grepl("w|e|C", DF_work$PREFIX),]

DF_work %>% mutate(PREFIX=str_to_upper(PREFIX)) -> 
  DF_work

DF_work$PREFIX[grepl("C", DF_work$PREFIX)] <- NA

DF_work %>% group_by(PREFIX) %>% tally(sort=TRUE)

```

## STREET_NAME

Just to be more comfortable, let's look at the street names that only occur once, or very few times.

```{r street name}

DF_work %>% group_by(STREET_NAME) %>% 
  tally(sort=TRUE) %>% 
  filter(n==1)

DF_work %>% group_by(STREET_NAME) %>% 
  tally(sort=TRUE) %>% 
  ggplot() +
    geom_histogram(aes(x = log(n)), fill = "red")  
  
```

Sigh. Many mispellings. As I feared. Does it matter? Maybe not. Some of the streets with 2 occurances are mispelled, some are not.

If I treat them all as real, I will still only match the valid names. The bad names will just be noise and are unlikely to match. On the other hand, if I don't fix the names, I may as well delete them since they will contribute nothing. But how do I know all the singlets are misspelled? Some could be short streets with a single building on them.

For now, I'll leave them.

## SUFFIX

N, S, E, W seem reasonable, but Y? and lower case "w".

```{r suffix}

DF_work %>% group_by(SUFFIX) %>% tally(sort=TRUE)

DF_work[grepl("w|Y", DF_work$SUFFIX),]

DF_work %>% mutate(SUFFIX=str_to_upper(SUFFIX)) -> 
  DF_work

DF_work$SUFFIX[grepl("Y", DF_work$SUFFIX)] <- NA

DF_work %>% group_by(SUFFIX) %>% tally(sort=TRUE)

foo <- DF_work
```

## STREET_TYPE

Way too many of these at 110, expect spelling and abbreviation errors.

Many instances where the latter part of a name got slotted into the type, e.g. "creek". Move these instances to the street name, and then look for duplicates. Look at lat/long diff < .0005 = 170 feet.

Actually, look for duplicate address/ first part of street name, and look at distances. Do this first.

OMG. I thought there was a 1:1 relationship between addresses and lat-long points, but for gated communities, they give every house in the community the same address. So for 12800 Briar Forest I have 186 records, each with a different lat-long, and, I think, each representing a different house. What to do?

I'm tempted to take the single record with the smallest objectid, assuming that is the oldest. That is what I will do. HCAD has the units split out with unit nyumbers, but HPD only uses the Briar Forest block range, and the city permit database seems also to use only the broad address and not the unit number. Perhaps I should use the median or mean value location?

Can I distinguish these from the actual duplicates? 

```{r street type}

DF_work %>% 
  group_by(STREET_TYPE) %>% 
  tally(sort=TRUE) %>% 
  arrange(STREET_TYPE)

# Look for duplicates

#DF_work %>% head(10000) %>% 
#  mutate(test_addy=paste(STREET_NUM, word(STREET_NAME), STREET_TYPE)) %>% 
#  group_by(test_addy) %>% 
#  filter(n()>1)

#   Erase value from STREET_TYPE and add to value to STREET_NAME

from_to <- c("BLF", "BLUFF",
             "BR", "BRANCH",
             "BEND", "BEND",
             "BRG", "BRIDGE",
             "BRKS", "BROOKS",
             "BROOK", "BROOK",
             "CENTER", "CENTER",
             "CLB", "CLUB",
             "COR", "CORNER",
             "CREEK", "CREEK",
             "CROSSING", "CROSSING",
             "CRST", "CREST",
             "CTS", "COURTS",
             "CYN", "CANYON",
             "DL", "DALE",
             "EST", "ESTATE",
             "ESTS", "ESTATE",
             "FALL", "FALL",
             "FLD", "FIELD",
             "FLS", "FALLS",
             "FRK", "",
             "FRST", "FOREST",
             "FRY", "FERRY CROSSING",
             "GARDEN", "GARDEN",
             "GARDENS", "GARDENS",
             "GLEN", "GLEN",
             "GREEN", "GREEN",
             "GROVE", "GROVE",
             "GRVS", "GROVE",
             "HBR", "HARBOR",
             "HILL", "HILL",
             "HLS", "HILLS",
             "HOLW", "HOLLOW",
             "HTS", "HEIGHTS",
             "HVN", "HAVEN",
             "INLT", "INLET",
             "IS", "ISLAND",
             "KNL", "KNOLL",
             "KNLS", "KNOLLS",
             "LK", "LAKE",
             "LKS", "LAKES",
             "LNDG", "LANDING",
             "MDW", "MEADOW",
             "MDWS", "MEADOWS",
             "ML", "MILL",
             "MLS", "MILLS",
             "MT", "MT",
             "MTN", "MOUNTAIN",
             "PLNS", "PLAINS",
             "PNE", "PINE",
             "PNES", "PINES",
             "POINT", "POINT",
             "PR", "PRAIRIE",
             "PRT", "PORT",
             "RIDGE", "RIDGE",
             "RIV", "RIVER",
             "RNCH", "RANCH",
             "ROW", "ROW",
             "RST", "REST",
             "SHR", "SHORE",
             "SHRS", "SHORES",
             "SPG", "SPRING",
             "SPWY", "SPEEDWAY",
             "STATION", "STATION",
             "STRM", "STREAM",
             "TOLLWAY", "COURT",
             "VLG", "VILLAGE",
             "VLY", "VALLEY",
             "VW", "VIEW",
             "WLS", "WELLS")

#     Create dictionary data frame of pattern/replacement
Makedict <- function(dictionary) {
  dict <- cbind.data.frame(split(dictionary, rep(1:2, times=length(dictionary)/2)), stringsAsFactors=F)
  names(dict) <- c("From", "To")
  return(dict)
}

dict <- Makedict(from_to)

#   test the searches first to see what they will find
testregex <- function(dframe, col, pat) { # input data frame and regex
  for(i in 1:length(pat[,1])) {
    print(paste("Pattern: ",pat[i,1]))
    hits <- unique(dframe[[col]][grepl(paste0("^",pat[i,1],"$"),dframe[[col]])])
    if (length(hits)>0){
      print(paste("   Result: ", hits))
    }
    else {
      print("No hits")
    }
  }
}

testregex(DF_work, "STREET_TYPE", dict)


#   apply to input array
applyregex <- function(dframe, col_from, pat, col_to) {
  for(i in 1:length(pat[,1])) {
    # if col_from == pat[i,1] &
    #      pat[i,2] not found in dframe[[col_to]] then
    #            append pat[i,2] to dframe[[col_to]]
    #     set col_from to NA
    #
    #print(paste(dframe[[col_to]], i, pat[i,1], dframe[[col_from]],
    #           pat[i,2], col_from, col_to))
    #   what will happen if col_from = NA?????
  dframe[[col_to]] <- if_else(pat[i,1]==dframe[[col_from]] &
                              !str_detect(dframe[[col_to]], pat[i,2]), 
                                  paste(dframe[[col_to]], pat[i,2]),
                                  dframe[[col_to]],
                                  missing=dframe[[col_to]])
  
  dframe[[col_from]][pat[i,1]==dframe[[col_from]]] <- "FIX ME"
  }
  return(dframe)
}

DF_work <- applyregex(DF_work, "STREET_TYPE", dict, "STREET_NAME")

#   Other errors to fix
# Orchard dale CIRCLE
mask1 <- str_detect(DF_work$STREET_NAME, "ORCHARD DALE")
mask2 <- DF_work$STREET_TYPE=="FIX ME"
sum(mask1&mask2)
DF_work$STREET_TYPE[mask1&mask2] <- "CIRCLE"
# Lynette Falls DR
mask1 <- str_detect(DF_work$STREET_NAME, "LYNETTE FALLS")
mask2 <- DF_work$STREET_TYPE=="FIX ME"
sum(mask1&mask2)
DF_work$STREET_TYPE[mask1&mask2] <- "DRIVE"
# Bend LANDING (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^BEND$")
mask2 <- DF_work$STREET_TYPE=="LDG"
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "BEND LANDING"
mask1 <- str_detect(DF_work$STREET_NAME, "^BEND LANDING$")
sum(mask1)
DF_work$STREET_TYPE[mask1] <- NA
# Forest LODGE (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^FOREST$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^LDG$")
sum(mask1&mask2, na.rm = TRUE)
DF_work$STREET_NAME[mask1&mask2] <- "FOREST LODGE"
DF_work$STREET_TYPE[mask1&mask2] <- "DRIVE"
# Hampton LODGE (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^HAMPTON$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^LDG$")
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "HAMPTON LODGE"
DF_work$STREET_TYPE[mask1&mask2] <- NA
# Indian LODGE (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^INDIAN$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^LDG$")
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "INDIAN LODGE"
DF_work$STREET_TYPE[mask1&mask2] <- "LANE"
# RD -> ROAD
mask2 <- str_detect(DF_work$STREET_TYPE, "^RD$")
sum(mask2, na.rm = TRUE)
DF_work$STREET_TYPE[mask2] <- "ROAD"
# E MEYER E -> MEYER
mask1 <- str_detect(DF_work$STREET_NAME, "^MEYER$")
mask2 <- str_detect(DF_work$PREFIX, "^E$")
sum(mask1&mask2, na.rm = TRUE)
DF_work$PREFIX[mask1&mask2] <- NA
DF_work$SUFFIX[mask1&mask2] <- NA
# TS -> STREET
mask2 <- str_detect(DF_work$STREET_TYPE, "^TS$")
sum(mask2, na.rm = TRUE)
DF_work$STREET_TYPE[mask2] <- "STREET"
# DE .. VL -> DEVILLE .. DR
mask1 <- str_detect(DF_work$STREET_NAME, "^DE$")
sum(mask1, na.rm = TRUE)
DF_work$STREET_NAME[mask1] <- "DEVILLE"
DF_work$STREET_TYPE[mask1] <- "DRIVE"
# MAGENTA SPRING .. WAYS -> EAGLES PERCH .. WAY
mask1 <- str_detect(DF_work$STREET_NAME, "^MAGENTA SPRING$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^WAYS$")
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "EAGLES PERCH"
DF_work$STREET_TYPE[mask1&mask2] <- "WAY"

# Now update the remaining "FIX ME" records, but first deal with duplicates

```

## Duplicates

We interrupt this program to delve into the duplicate address issue.

It looks like most of them are either gated communities, trailer parks, or business warehouse areas with multiple building numbers.

But to really understand them, I need to see a map. So let's map them.

Also look at mean, median, and relationship to SOURCE.


```{r Duplicates}

#   First let's look at likely duplicates and see if there are patterns,
#   especially regarding the STATUS

DF_work %>% 
  filter(STREET_NAME=="SHEPHERD") %>% 
  unite(groupaddress,STREET_NUMa, PREFIX, STREET_NAME, SUFFIX, CITY,
        sep=" ", na.rm=TRUE) %>% 
  group_by(groupaddress) %>% 
  tally(sort=TRUE) %>% 
  filter(n>1)
  
# Split dups off into a separate file so that I can plot them and see what they look like.

DF_work %>% 
  unite(groupaddress,STREET_NUMa, PREFIX, STREET_NAME, SUFFIX, STREET_TYPE, CITY,ZIPCODE,
        sep=" ", na.rm=TRUE, remove=FALSE) %>% 
  filter(groupaddress %in% unique(.[["groupaddress"]][duplicated(.[["groupaddress"]])])) -> 
           DF_dups

# Create file of unique values, I'll add the cleaned dups back later

DF_work %>% 
  unite(groupaddress,STREET_NUMa, PREFIX, STREET_NAME, SUFFIX, STREET_TYPE, CITY,ZIPCODE,
        sep=" ", na.rm=TRUE, remove=FALSE) %>%
  group_by(groupaddress) %>% 
  filter(n()==1) ->
  DF_Unique

DF_dups %>% group_by(SOURCES) %>% tally(sort=TRUE)

# Create mean, median locations plus max diff

lldiff <- function(df){
  latmean <- mean(df$Y)
  latmed <- median(df$Y)
  print(paste("latmean", latmean))
  lonmean <- mean(df$X)
  lonmed <- median(df$X)
  print(paste("lonmean", lonmean))
  maxdiffmean <- deg_to_ft*(max(sqrt((df$Y-latmean)**2 + (df$X-lonmean)**2)))
  maxdiffmed <- deg_to_ft*(max(sqrt((df$Y-latmed)**2 + (df$X-lonmed)**2)))
  if (nrow(df)>2) {
    maxindex <- which.max(deg_to_ft*(sqrt((df$Y-latmed)**2 + (df$X-lonmed)**2)))
    mask <- !logical(length=nrow(df)) # set all TRUE
    mask[maxindex] <- FALSE
    latmean <- mean(df$Y[mask])
    lonmean <- mean(df$X[mask])
    trimdiffmean <- deg_to_ft*(max(sqrt((df$Y[mask]-latmean)**2 + (df$X[mask]-lonmean)**2)))
    print(paste("-->",maxindex, mask, trimdiffmean))
  }
  else {trimdiffmean <- maxdiffmean}
  #print(names(df))
  #print(nrow(df))
  print(paste("maxdiff", maxdiffmean, df$key[1]))
  dfut <- data.frame(latmean, lonmean, latmed, lonmed, trimdiffmean, maxdiffmean, maxdiffmed, nrow(df))
  colnames(dfut) <- c("LatMean","LonMean",
                    "LatMed", "LonMed", "TrimDiffMean",
                    "RadMean", "RadMed" , "NumVals")
  return(dfut)
}

DF_dups %>% arrange(groupaddress) %>% 
  mutate(key=groupaddress) %>%  
  #head(10) %>% 
  group_by(groupaddress) %>% 
  group_modify(~ lldiff(.x)) -> Map_dups

#   Let's make a map

Map_dups %>% 
  filter(RadMean>1000) %>% 
  leaflet() %>% 
    setView(lat=29.757831, lng=-95.362680, zoom=12) %>% 
    addTiles() %>% 
    addCircles(lng=~LonMed, lat=~LatMed, radius=~RadMed/3.2, fillColor = "red") %>% 
    addCircles(lng=~LonMean, lat=~LatMean, radius=~RadMean/3.2, popup=~groupaddress) 

BadDups <- Map_dups %>% 
  filter(RadMean>500)

#    test taking the nearest points for 3 or more dups and recalculate error



```

Looks like most dups are either legitimate (gated communities or warehouses), or contain one point off from the others. 

For addresses with >= 3 points, throw out the point furthest from the rest and then take the mean location.

For addresses with 2 points, we have some issues. Some streets are missing the prefix (N, S, E, W) like Wellington. The esay way out would be to just drop all points with 2 dups and large radius. For 1000 feet, there are only 40 of those.

Let's test the reject algorithm and see what happens.

I like the trimmed mean, so I will use that for dups with 3 or more points.

Now lets pull out the dups of only 2 points, and plot up the distribution of the error.

A cutoff of about 250 looks reasonable. Less than 250, take average. Greater than 250, delete. Or really, push off into an error dataframe for potentially later work. 

```{r study dup pairs}

Map_dups %>% 
  filter(NumVals==2, RadMean<500) %>% 
  ggplot() +
      geom_histogram(aes(x = RadMean, fill = "red"))+
      ggtitle("Error values less than 500")


```

```{r Clean up duplicates}

# Calculate trimmed mean for 3 or more points
# Calculate the mean for 2 points
# If mean - pt > 250, then put into a special dataframe and delete from main
# For other variables, I need a hierarchy. 

llmean <- function(df){
  latmean <- mean(df$Y)
  lonmean <- mean(df$X)
  maxdiffmean <- deg_to_ft*(max(sqrt((df$Y-latmean)**2 + (df$X-lonmean)**2)))
  if (nrow(df)>2) { # trimmed mean
    maxindex <- which.max(deg_to_ft*(sqrt((df$Y-latmean)**2 + (df$X-lonmean)**2)))
    mask <- !logical(length=nrow(df)) # set all TRUE
    mask[maxindex] <- FALSE
    latmean <- mean(df$Y[mask])
    lonmean <- mean(df$X[mask])
    trimdiffmean <- deg_to_ft*(max(sqrt((df$Y[mask]-latmean)**2 + (df$X[mask]-lonmean)**2)))
    #print(paste("-->",maxindex, mask, trimdiffmean))
  }
  else {trimdiffmean <- maxdiffmean}
  X <- lonmean
  Y <- latmean
  sortorder <- c("COH Permanent Address", "County Address", "Abandon", 
                 "PRE",  "Reverse Geocode", "UTA", 
                 "Center Point Address", "Out Of Range")
  # choose based on STATUS in this order, largest OBJECTID next
  # Note: Center Point is the local utility, gas and electric provider
  # COH Permanent Address	1245257		
  # County Address	32581		
  # Abandon	4744		
  # PRE	5176		
  # Reverse Geocode	2529		
  # UTA	3227		
  # Center Point Address	153064		
  # Out Of Range
  rec <- df %>% 
    arrange(match(STATUS, sortorder), desc(OBJECTID))
  return(rec[1,])
}

# Duptest <- DF_dups %>% filter(groupaddress %in% c("12800 BRIAR FOREST DRIVE HOUSTON 77077","1803 W CLAY STREET HOUSTON 77019"))

DF_dups %>% arrange(groupaddress) %>% 
  mutate(key=groupaddress) %>%  
  group_by(groupaddress) %>% 
  group_modify(~ llmean(.x)) -> Duptest

# Add Duptest back to DF_Unique to creat a new, duplicate-free dataset.

DF_work <- bind_rows(DF_Unique, Duptest) %>% select(-key)



```

