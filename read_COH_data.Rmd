---
title: "Read COH data"
author: "Alan Jackson"
date: "January 5, 2020"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(leaflet)
library(rgeos)
library(sf)

path <- "/home/ajackson/Rprojects/Geocoding/"

deg_to_ft <-  363633.62 # feet per degree at 29 degrees

googlecrs <- 4326

googleproj4 <- "+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext  +no_defs"


knitr::opts_chunk$set(echo = TRUE)
```

## Read in csv file

```{r read in file}

DF <- read_csv(paste0(path, "COH_ADDRESS_POINTS__PDD.csv"),
               col_types = "dddddcccccccccccddccccccccccccccc")

```

##  Do some cleanup and recast some variables

a handful of zipcodes end in *
several addresses contain a letter at the end like "B"
etc

```{r look at data}

check_num <- function(name_a, a){paste("Range", name_a, toString(range(a, na.rm=TRUE), sep=", "), "number of NA's", sum(is.na(a)))}

check_char <- function(name_a, a){print(paste("Variable:", name_a))
  print(head(unique(a), 25))
  print(paste("Number of NA's:", sum(is.na(a))))
  print(paste("Number Unique:", length(unique(a))))}

check_notnum <- function(name_a, a){print(paste("Variable:", name_a, "non-numerics =", length(a[grepl("\\D",a)])))
  head(a[grepl("\\D",a)])
  }

check_num("X", DF$X)
print("-----------------------")
check_num("Y", DF$Y)
print("-----------------------")
check_num("OBJECTID", DF$OBJECTID)
print("-----------------------")
check_num("ID", DF$ID)
print("-----------------------")
check_num("ADDR_ID", DF$ADDR_ID)
print("-----------------------")

check_char("TYPES", DF$TYPES)
print("-----------------------")
check_char("STATUS", DF$STATUS)
print("-----------------------")
check_char("STREET_NUM", DF$STREET_NUM)
check_notnum("STREET_NUM", DF$STREET_NUM)
print("-----------------------")

check_char("FRACTION", DF$FRACTION)
print("-----------------------")
check_char("PREFIX", DF$PREFIX)
print("-----------------------")
check_char("STREET_NAME", DF$STREET_NAME)
print("-----------------------")
check_char("SUFFIX", DF$SUFFIX)
print("-----------------------")
check_char("STREET_TYPE", DF$STREET_TYPE)
print("-----------------------")
check_char("STATE", DF$STATE)
print("-----------------------")
check_char("CITY", DF$CITY)
print("-----------------------")
check_char("ZIPCODE", DF$ZIPCODE)
check_notnum("ZIPCODE", DF$ZIPCODE)
print("-----------------------")
check_num("X_COORD", DF$X_COORD)
print("-----------------------")
check_num("Y_COORD", DF$Y_COORD)
print("-----------------------")
check_char("LOT", DF$LOT)
print("-----------------------")
check_char("BLOCK", DF$BLOCK)
print("-----------------------")
check_char("SOURCES", DF$SOURCES)
print("-----------------------")

```

## Data Observations

X and Y seem more reliable than X_COORD and Y_COORD. The range is correct, and there are no NA's. But do they differ?

OBJECTID looks good - the range more than covers the number of records, and no NA's. Are they unique?

ID has lots of NA's. Not sure what it is for.

Not sure what ADDR_ID is, but no NA's.

TYPES possibly useful, but largely NA dominated. 90% NA.

STATUS seems important, no NA's. Need to figure out what it means.

STREET_NUM has only 1 NA (good!), but almost 10,000 entries with an appended character string like "A" or "#1".

FRACTION is all over the map. Probably not useful.

PREFIX has issues. Case problems, and what is "C"?

STREET_NAME is good - no NA's.

SUFFIX is okay except for "Y" and "w".

STREET_TYPE is a problem. Too many. 109 street types? Really?

STATE has one NA. Seriously?

CITY has a lot of entries - 46! But only 2 NA's.

ZIPCODE: no NA's but 14 "7737*".

X_COORD, Y_COORD range wrong, needs further checking.

LOT don't know what it is, but doesn't look useful, to me.

BLOCK is NA about half the time, but appears to designate blocks when present.

SOURCES nearly always present, but not sure how I would use it. It may tell which duplicate address to use.

Rest of the fields are internal things like date updated, who did the update, etc.

##  Data cleanup

Let's now go through and clean up the data. I'll just delete records which are unrecoverable (bad zipcode, street number NA, street number not a number? )

Also look at other issues that the summaries uncovered.

```{r cleaning one}

# drop fields I don't care about

DF_work <- DF %>% select(X, Y, OBJECTID, TYPES, STATUS,
                         STREET_NUM, PREFIX, STREET_NAME,
                         SUFFIX, STREET_TYPE, CITY,
                         ZIPCODE, X_COORD, Y_COORD, BLOCK, SOURCES)

# eliminate City, Street_number with NA

DF_work <- DF_work %>% filter(CITY!="NA", STREET_NUM!="NA")

# eliminate non-numeric zips

DF_work <- DF_work %>% filter(!grepl("\\D", ZIPCODE))


```

##  Look at coordinates

Let's look at X,Y vs X_COORD and Y_COORD. How different are they?

```{r Coordinatefest}

DF_work <- 
  DF_work %>% 
  filter(!is.na(X_COORD), !is.na(Y_COORD)) 
  
  DF_work %>% 
  mutate(xdiff=X-X_COORD, ydiff=Y-Y_COORD) %>% 
  select(X, Y, xdiff, ydiff) %>% 
  ggplot() +
    geom_histogram(aes(x = xdiff), fill = "red", alpha = 0.2) + 
    geom_histogram(aes(x = ydiff), fill = "blue", alpha = 0.2) 

  DF_work %>% 
  ggplot() +
    geom_histogram(aes(x = X_COORD), fill = "red", alpha = 0.2) + 
    geom_histogram(aes(x = Y_COORD), fill = "blue", alpha = 0.2) 
```

I see. X_COORD and Y_COORD are actual projected X,Y values, probably Lambert, while X,Y are actually lat, long values. I'll keep the projected coordinates for now. But delete the NA's.

## Objectid - unique?

```{r objectid}

n_distinct(DF_work$OBJECTID)
nrow(DF_work)

```

Yep. Unique. A very useful field.

## Status

Let's count up each status

```{r status counts}

DF_work %>% group_by(STATUS) %>% tally(sort=TRUE)

```

What do they mean?

COH Permanent, Center Point, and County are all good addresses. Maybe, Center Point is the power and gas company, and there addresses seem more often damaged or weird than the others. PRE means preliminary - probably will be made one of the first three, later. Abandon means used to be a valid address, but no longer is. For example, 2050 Main might have the building razed and replaced with two now at 2050A and 2050B. 2050 gets abandoned.

UTA - no clue. Seems to be valid, but not tied to a particular structure.

Reverse Geocode - Not sure, suspect it means "interpolated address".

Out of Range - really means out of sequence, e.g., 5,7,4,9 4 would be out of range. Drop these if doing any interpolation!

## STREET_NUM

What to do about those annoying extras added to the street numbers, like "A" or "#5"? Since my ultimate goal is to geocode by block range,
all I need to calculate is the block range, and something approximating
the center of the block. Should I use the average location, the median, or perhaps the midpoint between the min and max? I think initially I might calculate all three and compare, just to see how they each work. For all of these, I can drop the non-numeric and it will affect nothing. So that seems to be the answer.

```{r cleanup street number}

# remove anything following numerics that start with a non-numeric

DF_work %>% mutate(STREET_NUMa=str_extract(STREET_NUM, "^\\d+")) -> 
  DF_work

check_char("STREET_NUMa", DF_work$STREET_NUMa)
check_notnum("STREET_NUMa", DF_work$STREET_NUMa)

# Make STREET_NUMa numeric to ease later calculations

DF_work$STREET_NUMa <- as.numeric(DF_work$STREET_NUMa)

```

## PREFIX

This should be N, S, E, W, but also has w, e, and C as values. Clearly I just need to uppercase w and e, but what is "C"? C should be NA. I looked at the map for that street.

```{r prefix}

DF_work %>% group_by(PREFIX) %>% tally(sort=TRUE)

DF_work[grepl("w|e|C", DF_work$PREFIX),]

DF_work %>% mutate(PREFIX=str_to_upper(PREFIX)) -> 
  DF_work

DF_work$PREFIX[grepl("C", DF_work$PREFIX)] <- NA

DF_work %>% group_by(PREFIX) %>% tally(sort=TRUE)

```

## STREET_NAME

Just to be more comfortable, let's look at the street names that only occur once, or very few times.

```{r street name}

DF_work %>% group_by(STREET_NAME) %>% 
  tally(sort=TRUE) %>% 
  filter(n==1)

DF_work %>% group_by(STREET_NAME) %>% 
  tally(sort=TRUE) %>% 
  ggplot() +
    geom_histogram(aes(x = log(n)), fill = "red")  
  
```

Sigh. Many mispellings. As I feared. Does it matter? Maybe not. Some of the streets with 2 occurances are mispelled, some are not.

If I treat them all as real, I will still only match the valid names. The bad names will just be noise and are unlikely to match. On the other hand, if I don't fix the names, I may as well delete them since they will contribute nothing. But how do I know all the singlets are misspelled? Some could be short streets with a single building on them.

For now, I'll leave them.

## SUFFIX

N, S, E, W seem reasonable, but Y? and lower case "w".

```{r suffix}

DF_work %>% group_by(SUFFIX) %>% tally(sort=TRUE)

DF_work[grepl("w|Y", DF_work$SUFFIX),]

DF_work %>% mutate(SUFFIX=str_to_upper(SUFFIX)) -> 
  DF_work

DF_work$SUFFIX[grepl("Y", DF_work$SUFFIX)] <- NA

DF_work %>% group_by(SUFFIX) %>% tally(sort=TRUE)

foo <- DF_work
```

## STREET_TYPE

Way too many of these at 110, expect spelling and abbreviation errors.

Many instances where the latter part of a name got slotted into the type, e.g. "creek". Move these instances to the street name, and then look for duplicates. Look at lat/long diff < .0005 = 170 feet.

Actually, look for duplicate address/ first part of street name, and look at distances. Do this first.

OMG. I thought there was a 1:1 relationship between addresses and lat-long points, but for gated communities, they give every house in the community the same address. So for 12800 Briar Forest I have 186 records, each with a different lat-long, and, I think, each representing a different house. What to do?

I'm tempted to take the single record with the smallest objectid, assuming that is the oldest. That is what I will do. HCAD has the units split out with unit nyumbers, but HPD only uses the Briar Forest block range, and the city permit database seems also to use only the broad address and not the unit number. Perhaps I should use the median or mean value location?

Can I distinguish these from the actual duplicates? 

```{r street type}

DF_work %>% 
  group_by(STREET_TYPE) %>% 
  tally(sort=TRUE) %>% 
  arrange(STREET_TYPE)

# Look for duplicates

#DF_work %>% head(10000) %>% 
#  mutate(test_addy=paste(STREET_NUM, word(STREET_NAME), STREET_TYPE)) %>% 
#  group_by(test_addy) %>% 
#  filter(n()>1)

#   Erase value from STREET_TYPE and add to value to STREET_NAME

from_to <- c("BLF", "BLUFF",
             "BR", "BRANCH",
             "BEND", "BEND",
             "BRG", "BRIDGE",
             "BRKS", "BROOKS",
             "BROOK", "BROOK",
             "CENTER", "CENTER",
             "CLB", "CLUB",
             "COR", "CORNER",
             "CREEK", "CREEK",
             "CROSSING", "CROSSING",
             "CRST", "CREST",
             "CTS", "COURTS",
             "CYN", "CANYON",
             "DL", "DALE",
             "EST", "ESTATE",
             "ESTS", "ESTATE",
             "FALL", "FALL",
             "FLD", "FIELD",
             "FLS", "FALLS",
             "FRK", "",
             "FRST", "FOREST",
             "FRY", "FERRY CROSSING",
             "GARDEN", "GARDEN",
             "GARDENS", "GARDENS",
             "GLEN", "GLEN",
             "GREEN", "GREEN",
             "GROVE", "GROVE",
             "GRVS", "GROVE",
             "HBR", "HARBOR",
             "HILL", "HILL",
             "HLS", "HILLS",
             "HOLW", "HOLLOW",
             "HTS", "HEIGHTS",
             "HVN", "HAVEN",
             "INLT", "INLET",
             "IS", "ISLAND",
             "KNL", "KNOLL",
             "KNLS", "KNOLLS",
             "LK", "LAKE",
             "LKS", "LAKES",
             "LNDG", "LANDING",
             "MDW", "MEADOW",
             "MDWS", "MEADOWS",
             "ML", "MILL",
             "MLS", "MILLS",
             "MT", "MT",
             "MTN", "MOUNTAIN",
             "PLNS", "PLAINS",
             "PNE", "PINE",
             "PNES", "PINES",
             "POINT", "POINT",
             "PR", "PRAIRIE",
             "PRT", "PORT",
             "RIDGE", "RIDGE",
             "RIV", "RIVER",
             "RNCH", "RANCH",
             "ROW", "ROW",
             "RST", "REST",
             "SHR", "SHORE",
             "SHRS", "SHORES",
             "SPG", "SPRING",
             "SPWY", "SPEEDWAY",
             "STATION", "STATION",
             "STRM", "STREAM",
             "TOLLWAY", "COURT",
             "VLG", "VILLAGE",
             "VLY", "VALLEY",
             "VW", "VIEW",
             "WLS", "WELLS")

#     Create dictionary data frame of pattern/replacement
Makedict <- function(dictionary) {
  dict <- cbind.data.frame(split(dictionary, rep(1:2, times=length(dictionary)/2)), stringsAsFactors=F)
  names(dict) <- c("From", "To")
  return(dict)
}

dict <- Makedict(from_to)

#   test the searches first to see what they will find
testregex <- function(dframe, col, pat) { # input data frame and regex
  for(i in 1:length(pat[,1])) {
    print(paste("Pattern: ",pat[i,1]))
    hits <- unique(dframe[[col]][grepl(paste0("^",pat[i,1],"$"),dframe[[col]])])
    if (length(hits)>0){
      print(paste("   Result: ", hits))
    }
    else {
      print("No hits")
    }
  }
}

testregex(DF_work, "STREET_TYPE", dict)


#   apply to input array
applyregex <- function(dframe, col_from, pat, col_to) {
  for(i in 1:length(pat[,1])) {
    # if col_from == pat[i,1] &
    #      pat[i,2] not found in dframe[[col_to]] then
    #            append pat[i,2] to dframe[[col_to]]
    #     set col_from to NA
    #
    #print(paste(dframe[[col_to]], i, pat[i,1], dframe[[col_from]],
    #           pat[i,2], col_from, col_to))
    #   what will happen if col_from = NA?????
  dframe[[col_to]] <- if_else(pat[i,1]==dframe[[col_from]] &
                              !str_detect(dframe[[col_to]], pat[i,2]), 
                                  paste(dframe[[col_to]], pat[i,2]),
                                  dframe[[col_to]],
                                  missing=dframe[[col_to]])
  
  dframe[[col_from]][pat[i,1]==dframe[[col_from]]] <- "FIX ME"
  }
  return(dframe)
}

DF_work <- applyregex(DF_work, "STREET_TYPE", dict, "STREET_NAME")

#   Other errors to fix
# Orchard dale CIRCLE
mask1 <- str_detect(DF_work$STREET_NAME, "ORCHARD DALE")
mask2 <- DF_work$STREET_TYPE=="FIX ME"
sum(mask1&mask2)
DF_work$STREET_TYPE[mask1&mask2] <- "CIRCLE"
# Lynette Falls DR
mask1 <- str_detect(DF_work$STREET_NAME, "LYNETTE FALLS")
mask2 <- DF_work$STREET_TYPE=="FIX ME"
sum(mask1&mask2)
DF_work$STREET_TYPE[mask1&mask2] <- "DRIVE"
# Bend LANDING (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^BEND$")
mask2 <- DF_work$STREET_TYPE=="LDG"
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "BEND LANDING"
mask1 <- str_detect(DF_work$STREET_NAME, "^BEND LANDING$")
sum(mask1)
DF_work$STREET_TYPE[mask1] <- NA
# Forest LODGE (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^FOREST$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^LDG$")
sum(mask1&mask2, na.rm = TRUE)
DF_work$STREET_NAME[mask1&mask2] <- "FOREST LODGE"
DF_work$STREET_TYPE[mask1&mask2] <- "DRIVE"
# Hampton LODGE (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^HAMPTON$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^LDG$")
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "HAMPTON LODGE"
DF_work$STREET_TYPE[mask1&mask2] <- NA
# Indian LODGE (LDG)
mask1 <- str_detect(DF_work$STREET_NAME, "^INDIAN$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^LDG$")
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "INDIAN LODGE"
DF_work$STREET_TYPE[mask1&mask2] <- "LANE"
# RD -> ROAD
mask2 <- str_detect(DF_work$STREET_TYPE, "^RD$")
sum(mask2, na.rm = TRUE)
DF_work$STREET_TYPE[mask2] <- "ROAD"
# E MEYER E -> MEYER
mask1 <- str_detect(DF_work$STREET_NAME, "^MEYER$")
mask2 <- str_detect(DF_work$PREFIX, "^E$")
sum(mask1&mask2, na.rm = TRUE)
DF_work$PREFIX[mask1&mask2] <- NA
DF_work$SUFFIX[mask1&mask2] <- NA
# TS -> STREET
mask2 <- str_detect(DF_work$STREET_TYPE, "^TS$")
sum(mask2, na.rm = TRUE)
DF_work$STREET_TYPE[mask2] <- "STREET"
# DE .. VL -> DEVILLE .. DR
mask1 <- str_detect(DF_work$STREET_NAME, "^DE$")
sum(mask1, na.rm = TRUE)
DF_work$STREET_NAME[mask1] <- "DEVILLE"
DF_work$STREET_TYPE[mask1] <- "DRIVE"
# MAGENTA SPRING .. WAYS -> EAGLES PERCH .. WAY
mask1 <- str_detect(DF_work$STREET_NAME, "^MAGENTA SPRING$")
mask2 <- str_detect(DF_work$STREET_TYPE, "^WAYS$")
sum(mask1&mask2)
DF_work$STREET_NAME[mask1&mask2] <- "EAGLES PERCH"
DF_work$STREET_TYPE[mask1&mask2] <- "WAY"

# Now update the remaining "FIX ME" records, but first deal with duplicates

#   Save for easy restart
saveRDS(DF_work, paste(path, "In_Progress"))

```

## Duplicates

We interrupt this program to delve into the duplicate address issue.

It looks like most of them are either gated communities, trailer parks, or business warehouse areas with multiple building numbers.

But to really understand them, I need to see a map. So let's map them.

Also look at mean, median, and relationship to SOURCE.


```{r Duplicates}

#   First let's look at likely duplicates and see if there are patterns,
#   especially regarding the STATUS

DF_work %>% 
  filter(STREET_NAME=="SHEPHERD") %>% 
  unite(groupaddress,STREET_NUMa, PREFIX, STREET_NAME, SUFFIX, CITY,
        sep=" ", na.rm=TRUE) %>% 
  group_by(groupaddress) %>% 
  tally(sort=TRUE) %>% 
  filter(n>1)
  
# Split dups off into a separate file so that I can plot them and see what they look like.

DF_work %>% 
  unite(groupaddress,STREET_NUMa, PREFIX, STREET_NAME, SUFFIX, STREET_TYPE, CITY,ZIPCODE,
        sep=" ", na.rm=TRUE, remove=FALSE) %>% 
  filter(groupaddress %in% unique(.[["groupaddress"]][duplicated(.[["groupaddress"]])])) -> 
           DF_dups

# Create file of unique values, I'll add the cleaned dups back later

DF_work %>% 
  unite(groupaddress,STREET_NUMa, PREFIX, STREET_NAME, SUFFIX, STREET_TYPE, CITY,ZIPCODE,
        sep=" ", na.rm=TRUE, remove=FALSE) %>%
  group_by(groupaddress) %>% 
  filter(n()==1) ->
  DF_Unique

DF_dups %>% group_by(SOURCES) %>% tally(sort=TRUE)

# Create mean, median locations plus max diff

lldiff <- function(df){
  latmean <- mean(df$Y)
  latmed <- median(df$Y)
  #print(paste("latmean", latmean))
  lonmean <- mean(df$X)
  lonmed <- median(df$X)
  #print(paste("lonmean", lonmean))
  maxdiffmean <- deg_to_ft*(max(sqrt((df$Y-latmean)**2 + (df$X-lonmean)**2)))
  maxdiffmed <- deg_to_ft*(max(sqrt((df$Y-latmed)**2 + (df$X-lonmed)**2)))
  if (nrow(df)>2) {
    maxindex <- which.max(deg_to_ft*(sqrt((df$Y-latmed)**2 + (df$X-lonmed)**2)))
    mask <- !logical(length=nrow(df)) # set all TRUE
    mask[maxindex] <- FALSE
    latmean <- mean(df$Y[mask])
    lonmean <- mean(df$X[mask])
    trimdiffmean <- deg_to_ft*(max(sqrt((df$Y[mask]-latmean)**2 + (df$X[mask]-lonmean)**2)))
    #print(paste("-->",maxindex, mask, trimdiffmean))
  }
  else {trimdiffmean <- maxdiffmean}
  #print(names(df))
  #print(nrow(df))
  #print(paste("maxdiff", maxdiffmean, df$key[1]))
  dfut <- data.frame(latmean, lonmean, latmed, lonmed, trimdiffmean, maxdiffmean, maxdiffmed, nrow(df))
  colnames(dfut) <- c("LatMean","LonMean",
                    "LatMed", "LonMed", "TrimDiffMean",
                    "RadMean", "RadMed" , "NumVals")
  return(dfut)
}

DF_dups %>% arrange(groupaddress) %>% 
  #mutate(key=groupaddress) %>%  
  #head(10) %>% 
  group_by(groupaddress) %>% 
  group_modify(~ lldiff(.x)) -> Map_dups

#   Let's make a map

Map_dups %>% 
  filter(RadMean>1000) %>% 
  leaflet() %>% 
    setView(lat=29.757831, lng=-95.362680, zoom=12) %>% 
    addTiles() %>% 
    addCircles(lng=~LonMed, lat=~LatMed, radius=~RadMed/3.2, fillColor = "red") %>% 
    addCircles(lng=~LonMean, lat=~LatMean, radius=~RadMean/3.2, popup=~groupaddress) 

BadDups <- Map_dups %>% 
  filter(RadMean>500)

#    test taking the nearest points for 3 or more dups and recalculate error


#   Save for easy restart
saveRDS(DF_work, paste(path, "In_Progress"))

```

Looks like most dups are either legitimate (gated communities or warehouses), or contain one point off from the others. 

For addresses with >= 3 points, throw out the point furthest from the rest and then take the mean location.

For addresses with 2 points, we have some issues. Some streets are missing the prefix (N, S, E, W) like Wellington. The esay way out would be to just drop all points with 2 dups and large radius. For 1000 feet, there are only 40 of those.

Let's test the reject algorithm and see what happens.

I like the trimmed mean, so I will use that for dups with 3 or more points.

Now lets pull out the dups of only 2 points, and plot up the distribution of the error.

A cutoff of about 250 looks reasonable. Less than 250, take average. Greater than 250, delete. Or really, push off into an error dataframe for potentially later work. 

```{r study dup pairs}

Map_dups %>% 
  filter(NumVals==2, RadMean<500) %>% 
  ggplot() +
      geom_histogram(aes(x = RadMean, fill = "red"))+
      ggtitle("Error values less than 500")


```

```{r Clean up duplicates}

# Calculate trimmed mean for 3 or more points
# Calculate the mean for 2 points
# If mean - pt > 250, then put into a special dataframe and delete from main
# For other variables, I need a hierarchy. 

llmean <- function(df){
  latmean <- mean(df$Y)
  lonmean <- mean(df$X)
  maxdiffmean <- deg_to_ft*(max(sqrt((df$Y-latmean)**2 + (df$X-lonmean)**2)))
  if (nrow(df)>2) { # trimmed mean
    maxindex <- which.max(deg_to_ft*(sqrt((df$Y-latmean)**2 + (df$X-lonmean)**2)))
    mask <- !logical(length=nrow(df)) # set all TRUE
    mask[maxindex] <- FALSE
    latmean <- mean(df$Y[mask])
    lonmean <- mean(df$X[mask])
    trimdiffmean <- deg_to_ft*(max(sqrt((df$Y[mask]-latmean)**2 + (df$X[mask]-lonmean)**2)))
    #print(paste("-->",maxindex, mask, trimdiffmean))
  }
  else {trimdiffmean <- maxdiffmean}
  X <- lonmean
  Y <- latmean
  sortorder <- c("COH Permanent Address", "County Address", "Abandon", 
                 "PRE",  "Reverse Geocode", "UTA", 
                 "Center Point Address", "Out Of Range")
  # choose based on STATUS in this order, largest OBJECTID next
  # Note: Center Point is the local utility, gas and electric provider
  # COH Permanent Address	1245257		
  # County Address	32581		
  # Abandon	4744		
  # PRE	5176		
  # Reverse Geocode	2529		
  # UTA	3227		
  # Center Point Address	153064		
  # Out Of Range
  rec <- df %>% 
    arrange(match(STATUS, sortorder), desc(OBJECTID))
  return(rec[1,])
}

# Duptest <- DF_dups %>% filter(groupaddress %in% c("12800 BRIAR FOREST DRIVE HOUSTON 77077","1803 W CLAY STREET HOUSTON 77019"))

DF_dups %>% arrange(groupaddress) %>% 
#  mutate(key=groupaddress) %>%  
  group_by(groupaddress) %>% 
  group_modify(~ llmean(.x)) -> Duptest

# Add Duptest back to DF_Unique to creat a new, duplicate-free dataset.

#DF_work <- bind_rows(DF_Unique, Duptest) %>% select(-key)
DF_work <- bind_rows(DF_Unique, Duptest)

#   Save for easy restart
saveRDS(DF_work, paste(path, "In_Progress"))
```

## Fix me records

Okay, duplicate records are taken care of, now to the "FIX ME" records, all 9477 of them.

First let's clear out the noise. If all streets of a given STREET_NAME say "FIX ME", then just blank out the "FIX ME".

```{r uncontested fix me}

DF_work %>% 
  group_by(STREET_NAME) %>% 
  mutate(STREET_TYPE = case_when(
    all(STREET_TYPE == "FIX ME") ~ "",
    TRUE ~ STREET_TYPE
  )) %>%
  ungroup() -> DF_work

#   Fix PITA NA in type field

DF_work$STREET_TYPE[is.na(DF_work$STREET_TYPE)] <- ""

#   Save for easy restart
saveRDS(DF_work, paste(path, "In_Progress"))
```

##  Zipcode

Before going further, there are some addresses with incorrect zipcodes. These
will play havoc with the rest of the cleanup, so let's go after those now.

```{r clean zip}

#Zipcodes <- readRDS("~/Dropbox/CrimeStats/ZipCodes.rds")
Zipcodes <- readRDS("~/Rprojects/Geocoding/Zipcode_Polygons.rds")

# File has overlaps. How bad is it Johnny?

z <- st_make_valid(Zipcodes)

inter <- st_intersection(z) %>% filter(n.overlaps > 1)
plot(inter %>% select(ZIP_CODE))

#   Prep intersect file

dat <- data.frame(Longitude=DF_work$X, Latitude=DF_work$Y, Address=DF_work$groupaddress, stringsAsFactors = FALSE)

dat <- st_as_sf(dat, coords=c("Longitude", "Latitude"), crs=4326, agr = "identity")

#   find points in polygons
#   since zipcodes don't overlap, let's just grab the array index
#   instead of creating a huge matrix
#a <- st_intersects(dat, Zipcodes, sparse = TRUE)
a <- st_intersects(dat, z, sparse = TRUE)

#   Look for points missing a zipcode
dat[lengths(a)==0,]

#   Flag those points for later work

mask <- lengths(a)>0
sum(mask)
#a[lengths(a)==0] = 1
length(unlist(a)) # should be same length as mask

#   Append the ZIP field to the data frame
#DF_work$NewZip[mask] <- Zipcodes$Zip_Code[unlist(a)]
#DF_work$NewZip[mask] <- Zipcodes$ZIP_CODE[unlist(a)]
DF_work$NewZip[mask] <- z$ZIP_CODE[unlist(a)]

#   Look at all zipcode disagreements

ziperror <- DF_work %>% filter(ZIPCODE!=NewZip)

#   How far from being correct?

# Loop through zipcode file, and for each zipcode calc distance to
# points that are supposed to live in it
ziperror_sf <- st_as_sf(ziperror, coords=c("X", "Y"), crs=4326, agr = "identity")

for (zip in Zipcodes$ZIP_CODE){
  print(zip)
  mask <- ziperror_sf$ZIPCODE==zip
  if(sum(mask)>0) {
   ziperror$zipdistance[mask] <- 
     st_distance(ziperror_sf[mask,], Zipcodes[Zipcodes$ZIP_CODE==zip,])
  }
}

#   Let's plot a histogram of distances
ziperror %>% filter(!is.na(zipdistance)) %>% 
  filter(zipdistance<1000) %>% # drop distances greater than 1 km
  ggplot() +
  geom_histogram(aes(x = zipdistance), fill = "red")
  


```

## zipcode

A few points don't intersect the zipcode map. For some of these the 
coordinates are off enough that the point lives in Clear Lake, not
in a zipcode polygon.

About 2.5% of  points have an apparent wrong zipcode. Some have an
obviously mis-typed zipcode. Some have a coordinate that is off just
enough to push them across the boundary. So let's look at how close
to the correct zip they are.

Zipcode 77387 has an undefined distance from addresses, because it has no spatial location. It is used for PO Boxes only. So for the distance = NA, we will replace the zip with the calculated value.

Not happy with my zipcode polygons. They seem to be of not high quality. Let's try to do better. ESRI polygons win, but they are not perfect. For addresses "close" to the ESRI zipcode, we will use the
COH zipcode. But what is close? 

6919 YELLOWSTONE WAY DRIVE is off by 100 feet, but according to the Post Office, the COH zipcode is correct. The mapped zipcode boundary is simply wrong.

2424 FALCON PASS DRIVE is off by 200 feet, and is in fact incorrect.

I could look at streets of the same name to decide if the zip is wrong.

For that, let's assume that streets with more than 100 addresses, of which less than 2.5% have disagreements in the zip, and where the good zips are only one (to avoid streets where the street crosses two zipcodes), should have the ESRI zip.

Now let's replace zip's with greater than 5000 meters of error. That's another 4500 records.




```{r repair zips}

###########   Zips with NA for distance

#   Replace zip with ESRI zip

#   Make a list of OBJECTID's with NA distance and use to update DF_work

foo <- DF_work
BadZips <- ziperror %>% filter(is.na(zipdistance)) %>% 
  select(OBJECTID)
#   Update bad records
mask <- DF_work$OBJECTID %in% BadZips$OBJECTID
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

############   Zips with a crowdsource

#  If there are many more addresses with a good zip, and few with a #  bad zip, change the bad ones
foo <- DF_work

DF_work <- DF_work %>% 
  unite(FullStreet, PREFIX, STREET_NAME, SUFFIX, STREET_TYPE, CITY,
        sep=" ", na.rm=TRUE, remove=FALSE)

eraseme <- 
DF_work %>% group_by(FullStreet) %>% 
  mutate(NumGood=sum(ZIPCODE==NewZip), 
         Num=n(), 
         pct=NumGood/Num,
         uniq=n_distinct(NewZip)) %>% 
  ungroup() %>% 
  filter(Num>NumGood, uniq==1, NumGood>100, pct>0.9)

eraseme %>% 
  ggplot()+
  geom_histogram(aes(x=pct), bins=40)

#   Seems safe to use cutoffs of pct>0.975 and NumGood>100 with only
#   one unique NewZip

DF_work %>% group_by(FullStreet) %>% 
  mutate(NumGood=sum(ZIPCODE==NewZip), 
         Num=n(), 
         pct=NumGood/Num,
         uniq=n_distinct(NewZip)) %>% 
  ungroup() -> DF_work

mask <- DF_work$Num > DF_work$NumGood &
        DF_work$uniq == 1 &
        DF_work$NumGood > 100 &
        DF_work$pct > 0.975

sum(mask)

#  Apply the filter
DF_work$ZIPCODE[mask] <- DF_work$NewZip[mask]

###   These are the remaining records
eraseme <- 
DF_work %>% group_by(FullStreet) %>% 
   mutate(NumGood=sum(ZIPCODE==NewZip), 
         Num=n(), 
         pct=NumGood/Num,
         uniq=n_distinct(NewZip)) %>% 
  ungroup() %>%  
  filter(Num>NumGood, uniq==1, NumGood>100, pct>0.9)

foo <- DF_work
##############   Recalculate distance 
#     look for crowdsource with big distance

ziperror <- DF_work %>% filter(ZIPCODE!=NewZip)

ziperror_sf <- st_as_sf(ziperror, coords=c("X", "Y"), crs=4326, agr = "identity")

for (zip in Zipcodes$ZIP_CODE){
  print(zip)
  mask <- ziperror_sf$ZIPCODE==zip
  if(sum(mask)>0) {
   ziperror$zipdistance[mask] <- 
     st_distance(ziperror_sf[mask,], Zipcodes[Zipcodes$ZIP_CODE==zip,])
  }
}

#   Seems obvious that *all* the zips with an error greater than
#   5000 meters should just be replaced with the ESRI zip. So let's
#   do that.

BadZips <- ziperror %>% filter(zipdistance>=5000) %>% 
  select(OBJECTID)
#   Update bad records
mask <- DF_work$OBJECTID %in% BadZips$OBJECTID
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip


saveRDS(DF_work, paste(path, "In_Progress"))
foo <- DF_work

##############   Recalculate distance 
####   Let's tighten it a little.
####   distance > 500 m, pct>0.9, NumGood>100

ziperror <- DF_work %>% filter(ZIPCODE!=NewZip)

ziperror_sf <- st_as_sf(ziperror, coords=c("X", "Y"), crs=4326, agr = "identity")

for (zip in Zipcodes$ZIP_CODE){
  print(zip)
  mask <- ziperror_sf$ZIPCODE==zip
  if(sum(mask)>0) {
   ziperror$zipdistance[mask] <- 
     st_distance(ziperror_sf[mask,], Zipcodes[Zipcodes$ZIP_CODE==zip,])
  }
}

BadZips <- ziperror %>% 
  filter(zipdistance>500, NumGood>100, pct>0.9) %>% 
  select(OBJECTID)
#   Update bad records
mask <- DF_work$OBJECTID %in% BadZips$OBJECTID
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

foo <- DF_work

############  Now let's look at the low end of distance

ziperror <- DF_work %>% filter(ZIPCODE!=NewZip)

ziperror_sf <- st_as_sf(ziperror, coords=c("X", "Y"), crs=4326, agr = "identity")

for (zip in Zipcodes$ZIP_CODE){
  print(zip)
  mask <- ziperror_sf$ZIPCODE==zip
  if(sum(mask)>0) {
   ziperror$zipdistance[mask] <- 
     st_distance(ziperror_sf[mask,], Zipcodes[Zipcodes$ZIP_CODE==zip,])
  }
}

###ziperror <- ziperror %>% filter(zipdistance<100)

#   Let's look at the frequency of error by zip    

ziperror %>% ggplot() +
  geom_histogram(aes(x=ZIPCODE), stat="count")

#   Who are the biggies?

ziperror %>% group_by(ZIPCODE) %>% 
  mutate(Numerr=n()) %>% 
  ungroup() %>% 
  filter(Numerr>500) -> eraseme

#  77375 has 3374 possibly in error. Let's fix all of them

mask <- DF_work$OBJECTID %in% eraseme$OBJECTID[eraseme$ZIPCODE=="77375"]
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

#  77429 has 2508 possibly in error. Let's fix all of them with dist>100

mask <- DF_work$OBJECTID %in% eraseme$OBJECTID[eraseme$ZIPCODE=="77429" &
                 eraseme$zipdistance>100]
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

#  77449 has 1192 possibly in error. Let's fix all of them with dist>250

mask <- DF_work$OBJECTID %in% eraseme$OBJECTID[eraseme$ZIPCODE=="77449" &
                 eraseme$zipdistance>250]
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

#  77045 has 854 possibly in error. Let's fix all of them with dist>100

mask <- DF_work$OBJECTID %in% eraseme$OBJECTID[eraseme$ZIPCODE=="77045" &
                 eraseme$zipdistance>100]
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

#  77381 has 533 possibly in error. Let's fix all of them 

mask <- DF_work$OBJECTID %in% eraseme$OBJECTID[eraseme$ZIPCODE=="77381"]
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

#  77598 has 531 possibly in error. Let's fix all of them 

mask <- DF_work$OBJECTID %in% eraseme$OBJECTID[eraseme$ZIPCODE=="77598"]
sum(mask)
DF_work[mask,]$ZIPCODE <- DF_work[mask,]$NewZip

# 77054 ESRI map in error, distances useless here.

# It's about time to start geocoding with Google to compare

###########   stoppped here
```


Now what to do about streets that have a mix of "FIX ME" and other records?

Will need to look at several items to decide the proper course. STREET_NUMa should be in similar range, PREFIX and SUFFIX should match. CITY and ZIP hopefully will match. Let's start strictly and then see what is left.

Group by prefix, suffix, name, and zip. If address in range, then recode.

```{r strict recoding}

whattype <- function(type, num){
  #  If more than 2 types, bail
  if (n_distinct(type)>2) {return(type)}
  #  If only one type, bail
  if (n_distinct(type)==1) {return(type)}
  # If outside range of addresses, and not near, bail
  arange <- range(num[type!="FIX ME"]) # good address range
  fadd <- num[type=="FIX ME"] # FIX ME addresses
  # what is new type?
  #print(paste("arange", arange, "fadd", fadd))
  
  newtype <- type[type!="FIX ME"][1]
  if (between(fadd, arange[1], arange[2])){
    return(newtype)
  }
  else {return("FIX ME")}
  #return(newtype)
}

testme <- tribble(
  ~grp, ~type,  ~num,
  "a", "DRIVE", 100,
  "a", "FIX ME", 120,
  "a", "FIX ME", 130,
  "a", "FIX ME", 160,
  "a", "FIX ME", 90,
  "a", "DRIVE", 150,
  "b", "LANE", 150
)
testme %>% 
  group_by(grp) %>% 
  mutate(answer = case_when(
         sum(str_count(type,"FIX ME"))>0 ~ whattype(type, num),
         TRUE ~ type
  )) %>%
  ungroup()

testme %>% 
    group_by(grp) %>%
    mutate(in_range=if_else(between(num,
                               min(num[type!="FIX ME"]),
                               max(num[type!="FIX ME"])),
                          type[type!="FIX ME"][1], type
                          )) %>%  # is it in range? 
    ungroup() 


mymax <- function(...,def='hello world',na.rm=FALSE){
    if(!is.infinite(x<-suppressWarnings(max(...,na.rm=na.rm)))) x else def}
mymin <- function(...,def='hello world',na.rm=FALSE){
    if(!is.infinite(x<-suppressWarnings(min(...,na.rm=na.rm)))) x else def}

DF_work %>% 
  #head(20000) %>% 
  #select(-c(OBJECTID, TYPES, X_COORD, Y_COORD, BLOCK)) %>% 
  unite(grouper, PREFIX, STREET_NAME, SUFFIX, CITY,ZIPCODE,
        sep=" ", na.rm=TRUE, remove=FALSE) %>%
  group_by(grouper) %>% 
  mutate(STREET_TYPE2 = case_when(
      between(STREET_NUMa, mymin(STREET_NUMa[STREET_TYPE!="FIX ME"],def=0)-50,
                          mymax(STREET_NUMa[STREET_TYPE!="FIX ME"],def=0)+50)
              ~ STREET_TYPE[STREET_TYPE!="FIX ME"][1],
         TRUE ~ STREET_TYPE
  )) %>%
  ungroup() -> eraseme

```



```{r reprex}

#   If all values of key are bad, set value to blank, otherwise leave alone
reprexdata <- tribble(
  ~key, ~value,
  "a" , "good",
  "a" , "bad",
  "a" , "good",
  "b" , "bad",
  "b" , "bad",
  "b" , "bad",
  "c" , "good",
  "c" , "good",
  "c" , "good",
  "d" , "good",
  "d" , NA,
  "d" , "good"
)

# This fails because it clobbers the bad value for key=="a" with ifelse, or
# refuses to run using if_else
reprexdata %>% group_by(key) %>% 
  group_modify(~ {.x %>% mutate(
    value=if_else(nrow(.x)==sum(value=="bad", na.rm=TRUE), 
                 "", 
                 value))})

# This works, but seems a bit inelegant
reprexdata %>% group_by(key) %>% 
   group_modify(~ {.x %>% mutate(Number=nrow(.x), 
                                 Numbad=sum(value=="bad"))}) %>% 
  mutate(value=if_else(Number==Numbad, 
                      "", 
                      value)) %>% 
  select(-c(Number, Numbad))

reprexdata %>% 
  group_by(key) %>% 
  mutate(value = case_when(
    all(value == "bad") ~ "",
    TRUE ~ value
  )) %>%
  ungroup()

whattype <- function(type){
  #  If more than 2 types, bail
  if (n_distinct(type)>2) {return(type)}
  #  If only one type, bail
  if (n_distinct(type)==1) {return(type)}
  # what is new type?
  newtype <- type[type!="FIX ME"][1]
  return(newtype)
}

reprexdata %>% 
  group_by(key) %>% 
  mutate(newval = case_when(
         sum(str_count(value,"bad"))>0 ~ "foobar",
         TRUE ~ value
  )) %>%
  ungroup() 
```

